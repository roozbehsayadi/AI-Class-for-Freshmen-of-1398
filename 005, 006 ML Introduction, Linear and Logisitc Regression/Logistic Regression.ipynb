{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAHA Logsitics\n",
    "\n",
    "Ok sooo, I just learned linear legression. What the heck is logistic regression and what is the difference.\n",
    "\n",
    "Simply put, linear regression tries to fit **a line** which outputs the predictions as floats.\n",
    "\n",
    "But logistic regression is used for predicting categorical values and classification stuff. Like, is this picture a dog or a cat? What kind of fruit is this?\n",
    "\n",
    "The output is still a float value but we have to round it somehow so that it becomes the categorical value (int).\n",
    "\n",
    "The simplest form of the output would be binary classification. Only two kind of things to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer(return_X_y=True)\n",
    "x, y = data[0], data[1][:100].reshape(1, -1)\n",
    "x = x[:, 0][:100].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 100), (1, 100))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks different right? Because it is meant to be different.\n",
    "\n",
    "What we need now is a function. Not just any function. **The Logistic Function**. Here it is:\n",
    "\n",
    "$$\n",
    "Y = \\frac{1}{1 + e^-X}\n",
    "$$\n",
    "\n",
    "X and Y are not the data.\n",
    "\n",
    "**Note:** This is the simplest form of the logistic function. Check out wikipedia for more information.\n",
    "\n",
    "You might say \"Hold up! You said the prediction values are categorical; Now you're giving me this weird function which I know for sure the output isn't 0 or 1. What's happening?\"\n",
    "\n",
    "To that I will say, \"Patience, child.\"\n",
    "\n",
    "Ok, back to work. Let's see what does this function do to $[-10, 10)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_function = lambda x: 1 / (1 + exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_x = [i for i in range(-5, 5)]\n",
    "temp_y = [logistic_function(i) for i in temp_x]\n",
    "plt.scatter(temp_x, temp_y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that we will be using isn't just this function. There is more.\n",
    "\n",
    "Like linear regression, we define a function to generate output based on the input. We will be using the same $Y = mX + b$ function.\n",
    "\n",
    "After we passed our $X$ to the linear function, we need to calculate the **probability of the input belonging to a class**. Like how near is this output to class 0 or how near is it to class 1?\n",
    "\n",
    "For this, we use the logistic function. $Y$ is passed to the function and an output between $[0, 1]$ is given. Then we determine a limit (usually 0.5) and say, \"If output > 0.5 then it belongs to class 0. Otherwise, it belongs to class 1\".\n",
    "\n",
    "So the completed function would become this:\n",
    "\n",
    "$$\n",
    "p(class = 1) = [(\\frac{1}{1 + e^{-(mX + b)}}) > 0.5] \\\\\n",
    "p(class = 0) = [(\\frac{1}{1 + e^{-(mX + b)}}) < 0.5]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in linear regression, we have to define a function that we want to either minimize or maximize and do the \"predict, calculate new (better) coefficients\" again and again until we reach some bottleneck or our model is good enough (i.e. accuracy is higher than the desired value).\n",
    "\n",
    "So now we need to define a cost function. As before, without telling you why, let's consider this function as our $Cost$ function (not $Error$):\n",
    "\n",
    "$$\n",
    "Cost(\\hat Y, Y) = \\begin{cases}\n",
    "-log(\\hat Y), & \\text{if $Y = 1$}.\\\\\n",
    "-log(1 - \\hat Y), & \\text{if $Y = 0$}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Note:** For the updating and stuff, we delete don't consider the boundary and our $Y$ is the the function mentioned first in the notebook.\n",
    "\n",
    "Like linear, we want to minimize this. Let's simplify this a bit:\n",
    "\n",
    "$$\n",
    "Cost(\\hat Y, Y) = -Y\\log (\\hat Y) - (1 - Y)\\log(1 - \\hat Y)\n",
    "$$\n",
    "\n",
    "**Exercise:** Why this function? YOU should find out why folks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we can't just **find** the best value to $\\hat Y$ because the fucntion is **non-convex** and we can't just jump to the minimum and each time, we are trying to find the local minima. In each iteration, we add a value to the variables a.k.a **update** the variables until, like in linear, we reach a bottleneck or our model's accuract is good enough.\n",
    "\n",
    "Now there are various methods to solve this unconstrained optimization problem such as the 1st order method **gradient descent** that requires the gradient of the logistic regression cost function, or a 2nd order method such as **Newton’s method** that requires the **gradient** and the **Hessian** of the logistic regression cost function.\n",
    "\n",
    "Here, we want to use **gradient descent** which is defined as follows (for a dummy variable like m):\n",
    "\n",
    "$$\n",
    "m \\mathrel{-}= \\alpha \\frac{\\partial}{\\partial m} Cost(\\hat Y, Y)\n",
    "$$\n",
    "\n",
    "$\\alpha$ is called **learning rate** and it is manually determined at the beginning or the program or at the start of each iteration.\n",
    "\n",
    "For the sake of simplification, we split the derivation into three parts and use the chain rule to calculate the derivation (for variable m):\n",
    "\n",
    "$$\n",
    "\\frac {\\partial Cost}{\\partial m} = \\frac{\\partial Cost}{\\partial \\hat Y} \\ast \\frac{\\partial \\hat Y}{\\partial z} \\ast \\frac{\\partial z}{\\partial m}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat Y = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "z = mX + b\n",
    "$$\n",
    "\n",
    "**Exercise:** Calculate the derivation for both $m$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(m, x, b):\n",
    "    z = m * x + b\n",
    "    yhat = sigmoid(z)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(yhat, y):\n",
    "    first = -y * np.log(yhat)\n",
    "    second = (y - 1) * np.log(1 - yhat)\n",
    "    return np.sum(first + second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(m, x, b, y):\n",
    "    n = x.shape[1]\n",
    "    \n",
    "    # prediction\n",
    "    yhat = predict(m, x, b)\n",
    "    predict(m, x, b)\n",
    "    cst = cost(yhat, y) / n\n",
    "    \n",
    "    # gradient descent\n",
    "    dm = np.dot(x, (yhat - y).T) / n\n",
    "    db = np.sum(yhat - y) / n\n",
    "    \n",
    "    return dm, db, cst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(m, x, b, y, alpha=0.01, epochs=10):\n",
    "    costs = []\n",
    "    for i in range(epochs):\n",
    "        dm, db, cost = optimize(m, x, b, y)\n",
    "        \n",
    "        m -= (alpha * dm)\n",
    "        b -= (alpha * db)\n",
    "        \n",
    "        costs.append(cost)\n",
    "        \n",
    "    return m, b, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m, b, costs = model_train(0, x, 0, y)\n",
    "m = m[0][0]\n",
    "b = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.061982807651724946,\n",
       " array([[17.99 , 20.57 , 19.69 , 11.42 , 20.29 , 12.45 , 18.25 , 13.71 ,\n",
       "         13.   , 12.46 , 16.02 , 15.78 , 19.17 , 15.85 , 13.73 , 14.54 ,\n",
       "         14.68 , 16.13 , 19.81 , 13.54 , 13.08 ,  9.504, 15.34 , 21.16 ,\n",
       "         16.65 , 17.14 , 14.58 , 18.61 , 15.3  , 17.57 , 18.63 , 11.84 ,\n",
       "         17.02 , 19.27 , 16.13 , 16.74 , 14.25 , 13.03 , 14.99 , 13.48 ,\n",
       "         13.44 , 10.95 , 19.07 , 13.28 , 13.17 , 18.65 ,  8.196, 13.17 ,\n",
       "         12.05 , 13.49 , 11.76 , 13.64 , 11.94 , 18.22 , 15.1  , 11.52 ,\n",
       "         19.21 , 14.71 , 13.05 ,  8.618, 10.17 ,  8.598, 14.25 ,  9.173,\n",
       "         12.68 , 14.78 ,  9.465, 11.31 ,  9.029, 12.78 , 18.94 ,  8.888,\n",
       "         17.2  , 13.8  , 12.31 , 16.07 , 13.53 , 18.05 , 20.18 , 12.86 ,\n",
       "         11.45 , 13.34 , 25.22 , 19.1  , 12.   , 18.46 , 14.48 , 19.02 ,\n",
       "         12.36 , 14.64 , 14.62 , 15.37 , 13.27 , 13.45 , 15.06 , 20.26 ,\n",
       "         12.18 ,  9.787, 11.6  , 14.42 ]]),\n",
       " 0.002117504002833874,\n",
       " [0.6931471805599453,\n",
       "  0.6170704269439884,\n",
       "  0.6016876007425832,\n",
       "  0.5979360498285675,\n",
       "  0.5968940181706813,\n",
       "  0.5965684850475126,\n",
       "  0.5964456506553547,\n",
       "  0.5963822661748759,\n",
       "  0.5963366646423666,\n",
       "  0.5962964430471991])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, x, b, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit(yhat):\n",
    "    return (yhat >= 0.3).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: 0.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = limit(predict(m, x, b))\n",
    "\n",
    "print(\"The accuracy is: \" + str(accuracy_score(y_pred[0], y[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: 0.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "reg = LogisticRegression().fit(x[0].reshape(-1, 1), y[0])\n",
    "\n",
    "y_pred = reg.predict(x[0].reshape(-1, 1))\n",
    "\n",
    "print(\"The accuracy is: \" + str(accuracy_score(y_pred, y[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
